# -*- coding: utf-8 -*-
"""Loan_Prediction_ANN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zebw0NNnwA8MYd2mTkhpw2je0WJ37lXp

# Loan Prediction

## Importing the libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf

"""## Part-1 Data Preprocessing

### Importing the dataset
"""

dataset=pd.read_csv('Training Set.csv')
X=dataset.iloc[:, 1:-1].values
Y=dataset.iloc[:, -1].values

print(X)

"""### Taking care of missing data"""

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(missing_values=np.nan, strategy='most_frequent')
X=imputer.fit_transform(X)

print(X)

"""### Encoding Categorical Datas

#### Label Encoding
"""

from sklearn.preprocessing import LabelEncoder

le_1=LabelEncoder()
X[:, 0]=le_1.fit_transform(X[:, 0])

le_2=LabelEncoder()
X[:, 1]=le_2.fit_transform(X[:, 1])

le_3=LabelEncoder()
X[:, 3]=le_3.fit_transform(X[:, 3])

le_4=LabelEncoder()
X[:, 4]=le_4.fit_transform(X[:, 4])

le_Y=LabelEncoder()
Y=le_Y.fit_transform(Y)

"""#### One Hot Encoding"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct=ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [10])], remainder='passthrough')
X=np.array(ct.fit_transform(X))
print(X)

"""### Splitting the Dataset"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.2, random_state=0)

"""### Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

"""## Part-2 Building the ANN

### Initializing the ANN
"""

ann=tf.keras.models.Sequential()

"""### Adding the input layer and the first hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the second hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the output layer"""

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

"""## Part-3 Training the ANN

### Compiling the ANN
"""

ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""### Training the ANN on the Training Set"""

ann.fit(X_train, Y_train, batch_size=32, epochs=100)

"""## Part-4 Making the predictions and evaluating the model

### Predicting the Test Set Results
"""

Y_pred=ann.predict(X_test)>0.5
Y_pred=Y_pred.reshape(-1)
le_Y1=LabelEncoder()
Y_pred=le_Y1.fit_transform(Y_pred)
for i in range(0, len(Y_test)):
  print(Y_test[i], "  ", Y_pred[i])

"""### Making the Confusion Matrix"""

from sklearn.metrics import accuracy_score, confusion_matrix
cm=confusion_matrix(Y_test, Y_pred)
ac=accuracy_score(Y_test, Y_pred)
print(ac*100)